{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data\n",
    "This notebook runs through obtaining, filtering, and refining the raw data. It begins with a data dump of Reddit post submissions and extracts posts of the relevant subreddit: AskReddit. It thereby filters out insufficient entries (deleted posts, etc.) and extracts features of interest. Among these features is the sentiment of the post title, obtained through training a sentiment classifier on Twitter NLTK data (due to availability of manually classified datasets) with hopes that it will generalize to Reddit posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMP_DIRECTORY = 'https://files.pushshift.io/reddit/submissions/'\n",
    "DUMP_BASENAME = 'RS_2018-Y.xz'\n",
    "DATA_FILEPATH = '/home/ashuaibi7/project/data'\n",
    "ALL_POSTS = os.path.join(DATA_FILEPATH, 'ask_reddit_posts')\n",
    "GLOVE_HOME = os.path.join(DATA_FILEPATH, 'glove')\n",
    "SUBREDDIT_ID = 't5_2qh1i' # subreddit ID for AskReddit used in filtering\n",
    "FILE_COUNT = 6 # to fetch data dumps from months 1 - 6 of 2018\n",
    "RANDOM_SEED = 229"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I fetch the data dump files of reddit post submissions.\n",
    "# We fetch from pushshift.io, unzip, filter out posts from AskReddit,\n",
    "# then move the data to the respective directory. Finally, I concatenate\n",
    "# all AskReddit submissions into a single file.\n",
    "def fetch_data():\n",
    "    for i in range(1, FILE_COUNT + 1):\n",
    "        basename = DUMP_BASENAME.replace('Y', '{:02d}'.format(i))\n",
    "        base_name_split = os.path.splitext(basename)\n",
    "        dump_filename = os.path.join(DUMP_DIRECTORY, basename)\n",
    "        os.system('curl -LO {}'.format(dump_filename))\n",
    "        os.system('unxz {}'.format(basename))\n",
    "        os.system('grep \\\"{}\\\" {}> {}_ask_reddit'.format(SUBREDDIT_ID, base_name_split[0], base_name_split[0]))\n",
    "        os.system('mv -t {} {} {}_ask_reddit'.format(DATA_FILEPATH, base_name_split[0], base_name_split[0]))\n",
    "    all_files_basename = os.path.join(DATA_FILEPATH, '{}_ask_reddit'.format(os.path.splitext(basename)))\n",
    "    os.system('cat {}* > {}'.format(all_files_basename, ALL_POSTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we open and read the data. The file is contained of lines of \n",
    "# individual json entries, which we will parse using json.loads()\n",
    "# We parse columns of interested, including 'title', 'num_comments', and 'score'\n",
    "def process_posts():\n",
    "    data = []\n",
    "    with open(ALL_POSTS) as f:\n",
    "        for line in f:\n",
    "            data.append((json.loads(line)))\n",
    "    return pd.DataFrame(data)[['title', 'num_comments', 'created_utc', 'gilded', 'over_18', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = process_posts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Post Title Sentiment\n",
    "We will utilize NLTK Twitter data to develop a sentiment classifier. We thereby train on twitter data to the existence of manually classified sentiment datasets. We use the results of the model/weights to classify the sentiment of a reddit post title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the positive and negative tweets datasets and construct the \n",
    "# training and testing data\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "test_examples = positive_tweets[4000:] + negative_tweets[4000:]\n",
    "y = np.append(np.ones(1000), np.zeros(1000))\n",
    "positive_tweets = positive_tweets[:4000]\n",
    "negative_tweets = negative_tweets[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tweet tokenizer and stemmer and read in the english stopwords to utilize\n",
    "# in processing a single tweet.\n",
    "tweet_tokenizer = TweetTokenizer(reduce_len=True, preserve_case=False, strip_handles=True)\n",
    "stopwords_english = stopwords.words('english')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_tweet(tweet):\n",
    "    # remove the hashtags from word starts/ hyperlinks, and retweet sign before tokenizing\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    tweet_split = []\n",
    "    for word in tokens:\n",
    "        if (word not in stopwords_english): \n",
    "            tweet_split.append(stemmer.stem(word))\n",
    "    return tweet_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count of words that occur in positive and negative tweets respectively.\n",
    "def get_word_counts(positive_tweets, negative_tweets):\n",
    "    positive_counts, negative_counts, vocab = {}, {}, set()\n",
    "    for tweet in positive_tweets:\n",
    "        clean_tweet = process_single_tweet(tweet)\n",
    "        for word in clean_tweet:\n",
    "            vocab.add(word)\n",
    "            if word not in positive_counts.keys():\n",
    "                positive_counts[word] = 0\n",
    "            positive_counts[word] += 1\n",
    "    for tweet in negative_tweets:\n",
    "        clean_tweet = process_single_tweet(tweet)\n",
    "        for word in clean_tweet:\n",
    "            vocab.add(word)\n",
    "            if word not in negative_counts.keys():\n",
    "                negative_counts[word] = 0\n",
    "            negative_counts[word] += 1\n",
    "    return positive_counts, negative_counts, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_counts, negative_counts, vocab = get_word_counts(positive_tweets, negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_twitter_features(pos_counts, neg_counts, tweets):\n",
    "    X = np.zeros((len(tweets),2))\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        tweet = process_single_tweet(tweet) \n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        for word in tweet: \n",
    "            if word in pos_counts.keys():\n",
    "                pos_count += pos_counts[word]\n",
    "            if word in neg_counts.keys():\n",
    "                neg_count += neg_counts[word]\n",
    "        X[i,0], X[i,1] = pos_count, neg_count\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tweets = positive_tweets + negative_tweets\n",
    "Y = np.append(np.ones(len(positive_tweets)), np.zeros(len(negative_tweets)))\n",
    "X = extract_twitter_features(positive_counts, negative_counts, total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train with gradient descent\n",
    "def sigmoid(z): \n",
    "    return 1/(1 + np.exp(-z))\n",
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    m = x.shape[0]\n",
    "    for i in range(0,num_iters):\n",
    "        h = sigmoid(np.dot(x,theta))\n",
    "        J = -1 / m * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h)))\n",
    "        theta = theta - alpha* np.dot(x.T, h - y)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, alpha = np.zeros(3), 5e-15\n",
    "new_input = np.c_[np.ones(X.shape[0]), X]\n",
    "J, theta = gradientDescent(new_input, Y,theta, alpha, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9925"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x = extract_twitter_features(positive_counts, negative_counts, test_examples)\n",
    "new_x = np.c_[np.ones(new_x.shape[0]), new_x]\n",
    "pred = sigmoid(np.dot(new_x,theta)) > 0.5\n",
    "accuracy = 1 - np.abs(np.mean(y-pred))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Final Dataset\n",
    "We extract and engineer features for each AskReddit post and create the final dataset, dividing into train/dev/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First add a few features given the existing columns. Of these are the \n",
    "# length of the post 'title', the 'weekday' that a post was submitted,\n",
    "# and the 'hour' of the day the post was submitted.\n",
    "posts_df['title_length'] = posts_df['title'].str.len()\n",
    "posts_df['datetime'] = pd.to_datetime(posts_df['created_utc'], unit='s')\n",
    "posts_df['weekday'] = posts_df['datetime'].apply(lambda x: x.weekday())\n",
    "posts_df['hour'] = posts_df['datetime'].apply(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we classify the sentiment of a title and add that as a binary feature with 1 indicating\n",
    "# positive and 0 indicating negative. First extract features of the post titles\n",
    "def process_single_title(title):\n",
    "    tokens = word_tokenize(title)\n",
    "    title_split = []\n",
    "    for word in tokens:\n",
    "        if (word not in stopwords_english): \n",
    "            title_split.append(word)\n",
    "    return title_split\n",
    "\n",
    "def extract_reddit_features(pos_counts, neg_counts, titles):\n",
    "    classifications = []\n",
    "    for title in titles:\n",
    "        processed_title = process_single_title(title) \n",
    "        pos_count, neg_count = 0, 0\n",
    "        for word in processed_title: \n",
    "            if word in pos_counts.keys():\n",
    "                pos_count += pos_counts[word]\n",
    "            if word in neg_counts.keys():\n",
    "                neg_count += neg_counts[word]\n",
    "        new_x = np.array([1, pos_count, neg_count])\n",
    "        p = sigmoid(np.dot(theta.T, new_x))\n",
    "        classifications.append(1) if p > 0.5 else classifications.append(0)\n",
    "    return pd.Series(classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = extract_reddit_features(positive_counts, negative_counts, posts_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the 300D glove representations of the words in the title stacked together as another feature\n",
    "def glove_to_dict(filename):\n",
    "    data = {}\n",
    "    with open(filename) as f:\n",
    "        while True:\n",
    "            try:\n",
    "                line = next(f)\n",
    "                line = line.strip().split()\n",
    "                data[line[0]] = np.array(line[1: ], dtype = np.float)\n",
    "            except StopIteration: break\n",
    "            except UnicodeDecodeError: pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_lookup = glove_to_dict(os.path.join(GLOVE_HOME, 'glove.6B.300d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsm_title_phi(title, lookup, np_func=np.sum):\n",
    "    allvecs = np.array([lookup[w] for w in process_single_title(title) if w in lookup])    \n",
    "    if len(allvecs) == 0:\n",
    "        dim = len(next(iter(lookup.values())))\n",
    "        feats = np.zeros(dim)\n",
    "    else:       \n",
    "        feats = np_func(allvecs, axis=0)      \n",
    "    return feats\n",
    "def glove_title_phi(title, np_func=np.sum):\n",
    "    return vsm_title_phi(title, glove_lookup, np_func=np_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stacked_gloves(titles):\n",
    "    representations = []\n",
    "    for title in titles:\n",
    "        representations.append(glove_title_phi(title))\n",
    "    return pd.Series(representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_representations = extract_stacked_gloves(posts_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df['stacked_glove_representations'] = glove_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we reproducibly separate the data set randomly into train/dev/test\n",
    "def build_dataset(df):\n",
    "    df.to_csv(os.path.join(DATA_FILEPATH, 'all.csv'), index=False)\n",
    "    train = df.sample(frac=0.8, random_state=RANDOM_SEED)\n",
    "    remainder = df.drop(train.index)\n",
    "    test = remainder.sample(frac=0.5, random_state=RANDOM_SEED)\n",
    "    dev = remainder.drop(test.index)\n",
    "    train.to_csv(os.path.join(DATA_FILEPATH, 'train.csv'), index=False)\n",
    "    dev.to_csv(os.path.join(DATA_FILEPATH, 'dev.csv'), index=False)\n",
    "    test.to_csv(os.path.join(DATA_FILEPATH, 'test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will concern ourselves with a subset of the available features, namely:\n",
    "columns = ['num_comments', \n",
    "           'title_length', \n",
    "           'sentiment', \n",
    "           'weekday', \n",
    "           'hour', \n",
    "           'gilded',\n",
    "           'over_18',\n",
    "           'score', \n",
    "#            'stacked_glove_representations',\n",
    "           'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dataset(posts_df[columns])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
